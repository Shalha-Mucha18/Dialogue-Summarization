# Dialogue Summarization with FLAN-T5

## Overview
This repository contains code for the dialogue summarization task using a large language model (LLM), specifically FLAN-T5. We leverage the FLAN-T5 model from Hugging Face to perform summarization tasks on dialogue data.

## Task Description
The task involves summarizing dialogues using FLAN-T5. We explore various inference strategies, including zero-shot, one-shot, and few-shot, to understand their impact on the generative output of the language model. This project serves as a preliminary investigation into prompt engineering techniques for enhancing the summarization capabilities of LLMs.

## Dependencies
- Python 3
- PyTorch
- Transformers library from Hugging Face

## Usage
Follow the instructions in the notebook to:
- Load the FLAN-T5 model from Hugging Face.
- Preprocess dialogue data for summarization.
- Experiment with zero-shot, one-shot, and few-shot inference techniques.
- Evaluate the generated summaries.

## Results
The notebook includes experimental results and qualitative analysis of the summarization outputs obtained through different inference strategies.

